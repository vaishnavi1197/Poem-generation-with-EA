{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jennifergutman/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import syllables \n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import re\n",
    "import language_tool_python\n",
    "from IPython.display import display\n",
    "import nltk\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize\n",
    "import timeit\n",
    "from decimal import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import language_tool_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool = language_tool_python.LanguageTool('en-US')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/jennifergutman/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install gingerit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Dataset_poems.txt\", \"r\",encoding=\"utf8\") as file:\n",
    "    # Trial_data.txt\n",
    "    # Dataset_poems.txt\n",
    "    temp = []\n",
    "    for line in file:\n",
    "        line1 = re.sub('[^a-zA-Z ]', '', line)\n",
    "        line1 = line1.split()\n",
    "        if line1 != ['', ''] and line1 != ['']:\n",
    "            for i in line1:\n",
    "                if i == '':\n",
    "                    continue\n",
    "                else:\n",
    "                    temp.append(i.lower())\n",
    "words_df = temp[:10000] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_tag = pos_tag(words_df)\n",
    "dic_tag = {}\n",
    "for tup in tokens_tag:\n",
    "  if tup[1] not in dic_tag.keys():\n",
    "    dic_tag[tup[1]] = [tup[0]]\n",
    "  else:\n",
    "    temp = dic_tag[tup[1]] + [tup[0]]\n",
    "    dic_tag[tup[1]] = temp\n",
    "# print(dic_tag.keys())\n",
    "z = list(dic_tag.values())\n",
    "words_df2 = []\n",
    "for i in range(len(z)):\n",
    "  words_df2.extend(z[i])\n",
    "# print(words_df2)\n",
    "words_df = words_df2\n",
    "words_df.remove('o')\n",
    "words_df.remove('n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = []\n",
    "for i in range(3, len(words_df)-3):\n",
    "    lines.append(words_df[i-3]+' '+' '+words_df[i-2]+' '+' '+words_df[i-1]+' '+' '+words_df[i]+' '+' '+words_df[i+1]+' '+' '+words_df[i+2]+' '+' '+words_df[i+3])\n",
    "\n",
    "# sklearn countvectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# Convert a collection of text documents to a matrix of token counts\n",
    "cv = CountVectorizer(ngram_range=(1,1), stop_words = stopwords.words('english'))\n",
    "# matrix of token counts\n",
    "X = cv.fit_transform(lines)\n",
    "Xc = (X.T * X) # matrix manipulation\n",
    "Xc.setdiag(0) # set the diagonals to be zeroes as it's pointless to be 1\n",
    "\n",
    "names = cv.get_feature_names() # This are the entity names (i.e. keywords)\n",
    "cooccur_df = pd.DataFrame(data = Xc.toarray(), columns = names, index = names)\n",
    "total = cooccur_df.to_numpy().sum()\n",
    "cooccur_df = cooccur_df.div(total)\n",
    "cooccur_df.to_csv('co_occur_mat.csv', sep = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "class poem_generation:\n",
    "    pd.set_option('display.max_rows', None)\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    pd.set_option('display.width', None)\n",
    "    pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "    def select_random_words(self,words_df):\n",
    "        words_array1 = []\n",
    "        while len(words_array1) <= 15:\n",
    "            random_number = random.randint(0, len(words_df)-1)\n",
    "            if words_df[random_number] != 0:\n",
    "                words_array1.append(words_df[random_number])\n",
    "        # print(len(words_array1))\n",
    "\n",
    "        return words_array1\n",
    "\n",
    "    def create_poems(self,arr):\n",
    "        poem = [0 for _ in range(4)]\n",
    "        for i in range(1,5):\n",
    "            temp =  arr[4*(i-1):4*i]\n",
    "            poem[i-1] = temp\n",
    "            # print(\"HERE:\",temp)\n",
    "            # print(type(temp))\n",
    "            poem[i-1]=' '.join(poem[i-1])\n",
    "            # print(poem[i-1])\n",
    "        # print(\"=========================================================\")\n",
    "        return poem\n",
    "\n",
    "    def store_syllables(self,arr):\n",
    "        a = [0 for _ in range(5)]\n",
    "        temp = 0\n",
    "        j = 0\n",
    "        for words,i in zip(arr,range(0,16)):\n",
    "            temp = temp + syllables.estimate(words)\n",
    "            if i in [3,7,11,15]:\n",
    "                a[j] = temp\n",
    "                # print(a[j])\n",
    "                temp = 0\n",
    "                j += 1\n",
    "        \n",
    "        if a[0] == 5:\n",
    "            a[4] += 1\n",
    "        if a[1] == 6:\n",
    "            a[4] += 1\n",
    "        if a[1] == 6:\n",
    "            a[4] += 1\n",
    "        if a[3] == 5:\n",
    "            a[4] += 1\n",
    "        #a[4] = a[4] / 4\n",
    "        return a\n",
    "\n",
    "    def Grammar_checker(self,text):\n",
    "        error = 0\n",
    "        poem_error = []\n",
    "        # tool = language_tool_python.LanguageTool('en-US')\n",
    "        for lines in text:\n",
    "            matches = tool.check(lines)\n",
    "            if len(matches)>0:\n",
    "                error = error + matches[0].errorLength\n",
    "        poem_error.append(error)\n",
    "        return poem_error\n",
    "\n",
    "    def crossover(self,p1,p2,cross):\n",
    "        part1 = []\n",
    "        part2 = []\n",
    "        part3 = []\n",
    "        child1 = []\n",
    "        child2 = []\n",
    "        j = 0\n",
    "        k = 0\n",
    "        random_number1 = sorted([random.randint(1, 15) for i in range(cross)])\n",
    "        random_number2 = sorted([random.randint(1, 15) for i in range(cross)])\n",
    "        for i in random_number1:\n",
    "            part1.append(p1[j:i])\n",
    "            j = i\n",
    "        for i in random_number2:\n",
    "            part2.append(p2[k:i])\n",
    "            k = i\n",
    "        part1.append(p1[j:])\n",
    "        part2.append(p2[k:])\n",
    "        part2.extend(part1)\n",
    "        random.shuffle(part2)\n",
    "        for i in range(len(part2)):\n",
    "            part3.extend(part2[i])\n",
    "        for i in range(int(len(part3)/2)):\n",
    "            child1.append(part3[i])\n",
    "        for i in range(int(len(part3)/2),int(len(part3))):\n",
    "            child2.append(part3[i])\n",
    "        # if child1 == p1 or child1 == p2 or child2 == p1 or child2 == p2:\n",
    "        #     child1, child2,child1_poem,child2_poem = self.crossover(child1,child2,cross)\n",
    "        child1_poem = self.create_poems(child1)\n",
    "        child2_poem = self.create_poems(child2)\n",
    "        return child1,child2,child1_poem,child2_poem\n",
    "    \n",
    "    def coherence_checker(self, text):\n",
    "        clean_text = ''\n",
    "        for i in text:\n",
    "            clean_text = clean_text + ' ' + i\n",
    "        \n",
    "        text_tokens = word_tokenize(clean_text)\n",
    "        tokens_without_sw = [word for word in text_tokens if not word in stopwords.words('english')]\n",
    "        \n",
    "        coherence_val = 0\n",
    "        pairs = 0\n",
    "        for i in range(len(tokens_without_sw) - 1):\n",
    "            for j in range(i+1, len(tokens_without_sw)):\n",
    "                coherence_val += cooccur_df[tokens_without_sw[i]][tokens_without_sw[j]]\n",
    "                pairs += 1\n",
    "        \n",
    "        if (pairs > 0):\n",
    "            coherence_val /= pairs\n",
    "        #coherence_val\n",
    "        return [coherence_val]\n",
    "\n",
    "    def tournament(self,Population,m,best):\n",
    "        from_best = len(best)\n",
    "        # print(\"best:\",best)\n",
    "        except_best = m - from_best\n",
    "        # df = pd.DataFrame()\n",
    "        #selected_polulation = pd.DataFrame(columns=[\"Line1\",\"Line2\",\"Line3\",\"Line4\",\"Syllables1\",\"Syllables2\",\"Syllables3\",\"Syllables4\",\"GrammarError\",\"word_array\"])\n",
    "        ######################### when no syllables rule\n",
    "        # selected_polulation = Population.iloc[:nn]\n",
    "        # random_number = random.sample(list(Population.iloc[nn:].index),p)\n",
    "        # Population.sort_values(by=['GrammarError'], inplace=True)\n",
    "        # Population2 = Population[~Population.isin(selected_polulation).all(1)]\n",
    "        ######################### when syllables rule\n",
    "        if len(best) >0:\n",
    "            selected_polulation = []\n",
    "            words_array_selected = []\n",
    "            for b in best:\n",
    "                c = Population.iloc[b].values[:-1]\n",
    "                d = Population.iloc[b].values[-1]\n",
    "                selected_polulation.append(c)\n",
    "                words_array_selected.append(d)\n",
    "            words_array_selected = pd.DataFrame(dict(zip([\"words_array\"],[words_array_selected])))\n",
    "            \n",
    "############# This line was changed with addition of fitness column \n",
    "            #selected_polulation = pd.DataFrame(selected_polulation,columns=[\"Line1\",\"Line2\",\"Line3\",\"Line4\",\"Syllables1\",\"Syllables2\",\"Syllables3\",\"Syllables4\",\"GrammarError\",\"Coherence\"])\n",
    "            selected_polulation = pd.DataFrame(selected_polulation,columns=[\"Line1\",\"Line2\",\"Line3\",\"Line4\",\"Syllables1\",\"Syllables2\",\"Syllables3\",\"Syllables4\",\"Syll_Count\",\"GrammarError\",\"Coherence\",\"Gram Rank\",\"Coh Rank\",\"Fitness\"])\n",
    "            \n",
    "            selected_polulation = pd.concat([selected_polulation,words_array_selected],axis=1)\n",
    "            # print(\"poems selected by user\")\n",
    "            # display(selected_polulation)\n",
    "            Population2 = Population[~Population.isin(selected_polulation).all(1)]\n",
    "            Population2 = Population2.sort_values(by=['Fitness'], ascending = False)\n",
    "            selected_polulation2 = Population2\n",
    "            # selected_polulation2 = Population2[(Population2[\"Syllables1\"] >= \"3\")]\n",
    "            # print(\"grammar sorted poems by syll\")\n",
    "            # display(selected_polulation2)\n",
    "            from_sy = len(selected_polulation2.index)\n",
    "            if from_sy >= int(except_best/2):\n",
    "                from_sy = int(except_best/2)\n",
    "            else:\n",
    "                from_sy = from_sy\n",
    "            from_random = except_best - from_sy\n",
    "            selected_polulation2 = selected_polulation2.iloc[:from_sy]\n",
    "            selected_polulation = pd.concat([selected_polulation,selected_polulation2],axis=0)\n",
    "            selected_polulation = selected_polulation.reset_index(drop=True)\n",
    "            display(selected_polulation)\n",
    "            display(Population2)\n",
    "            Population2 = Population2[~Population2.isin(selected_polulation).all(1)]\n",
    "            Population2 = Population2.reset_index(drop=True)\n",
    "            selected_polulation = pd.concat([selected_polulation,Population2.iloc[:from_random]],axis=0)\n",
    "            selected_polulation = selected_polulation.reset_index(drop=True)\n",
    "            # print(\"SELECTED\")\n",
    "            # display(selected_polulation)\n",
    "            words_array_selected = list(selected_polulation[\"words_array\"])\n",
    "            # print(\"TYPE\",type(selected_polulation[\"words_array\"]),words_array_selected)\n",
    "            return selected_polulation,words_array_selected\n",
    "        else:\n",
    "            Population2 = Population.sort_values(by=['Fitness'], ascending = False)\n",
    "            random = int(except_best/3)\n",
    "            grammar = except_best - random\n",
    "            selected_polulation = Population2[:grammar]\n",
    "            Population2 = Population2[~Population2.isin(selected_polulation).all(1)]\n",
    "            selected_polulation2 = Population2[:random]\n",
    "            selected_polulation = pd.concat([selected_polulation,selected_polulation2],axis=0)\n",
    "            selected_polulation = selected_polulation.reset_index(drop=True)\n",
    "            # print(\"SELECTED\")\n",
    "            # display(selected_polulation)\n",
    "            words_array_selected = list(selected_polulation[\"words_array\"])\n",
    "            # print(\"TYPE\",type(selected_polulation[\"words_array\"]),words_array_selected)\n",
    "            return selected_polulation,words_array_selected\n",
    "\n",
    "\n",
    "\n",
    "    def create_set_of_poems(self,n):\n",
    "            poem_population = []\n",
    "            syllables_count = []\n",
    "            grammar_error = []\n",
    "            words_array_population = []\n",
    "            coherence_arr = []\n",
    "            for i in range(n):\n",
    "                words_array = self.select_random_words(words_df)\n",
    "                poem = self.create_poems(words_array)\n",
    "                sum_syllables = self.store_syllables(words_array)\n",
    "                poem_error = self.Grammar_checker(poem)\n",
    "                coherence = self.coherence_checker(poem)\n",
    "                poem_population.append(poem)\n",
    "                syllables_count.append(sum_syllables)\n",
    "                grammar_error.append(poem_error)\n",
    "                coherence_arr.append(coherence)\n",
    "                words_array_population.append(words_array)\n",
    "            words_array2 = pd.DataFrame(dict(zip([\"words_array\"],[words_array_population])))\n",
    "            Final = np.hstack((poem_population,syllables_count,grammar_error,coherence_arr))\n",
    "            Population_data = pd.DataFrame(Final,columns=[\"Line1\",\"Line2\",\"Line3\",\"Line4\",\"Syllables1\",\"Syllables2\",\"Syllables3\",\"Syllables4\",\"Syll_Count\",\"GrammarError\",\"Coherence\"])\n",
    "            \n",
    "############# Calculate Fitness of Poems\n",
    "            max_rank = len(Population_data[\"Line1\"])\n",
    "#             Population_data[\"Syllables1\"] = Population_data[\"Syllables1\"].apply(lambda x: [int(el) for el in x.strip(\"[]\").split(\",\")])\n",
    "#             Population_data[\"Syllables2\"] = Population_data[\"Syllables2\"].apply(lambda x: [int(el) for el in x.strip(\"[]\").split(\",\")])\n",
    "#             Population_data[\"Syllables3\"] = Population_data[\"Syllables3\"].apply(lambda x: [int(el) for el in x.strip(\"[]\").split(\",\")])\n",
    "#             Population_data[\"Syllables4\"] = Population_data[\"Syllables4\"].apply(lambda x: [int(el) for el in x.strip(\"[]\").split(\",\")])\n",
    "            Population_data[\"Syll_Count\"] = Population_data[\"Syll_Count\"].apply(lambda x: [float(el) for el in x.strip(\"[]\").split(\",\")])\n",
    "            Population_data[\"GrammarError\"] = Population_data[\"GrammarError\"].apply(lambda x: [int(el) for el in x.strip(\"[]\").split(\",\")])\n",
    "            Population_data[\"Gram_Rank\"] = Population_data[\"GrammarError\"].rank(method = 'min', ascending = False)\n",
    "            Population_data[\"Coherence\"] = Population_data[\"Coherence\"].apply(lambda x: [float(el) for el in x.strip(\"[]\").split(\",\")])\n",
    "            Population_data[\"Coh_Rank\"] = Population_data[\"Coherence\"].rank(method = 'min')\n",
    "            #Population_data = Population_data.assign(Syll_Bonus = lambda row: 1 if (int(row[\"Syllables1\"][0]) == 5) else 0)\n",
    "            #Population_data[\"Syll_Bonus\"] = Population_data.apply(lambda row: 1 if ((row.Syllables1[0] == 5)&(row.Syllables2[0] == 6)&(row.Syllables3[0] == 6)&(row.Syllables4[0] == 5)) else 0, axis = 1)\n",
    "            print(Population_data)\n",
    "            print(type(Population_data.at[1,\"Syll_Count\"]))\n",
    "            Population_data[\"Fitness\"] = Population_data.apply(lambda row: 0.4*row.Gram_Rank/max_rank + 0.4*row.Coh_Rank/max_rank + 0.2*row.Syll_Count/4, axis = 1)\n",
    "            Population_data = Population_data.explode(\"Coherence\")\n",
    "            Population_data = Population_data.explode(\"GrammarError\")\n",
    "            \n",
    "            #selected_pops = Population_data[(Population_data[\"Syllables1\"] == \"5\")|(Population_data[\"Syllables2\"] == \"6\")|(Population_data[\"Syllables3\"] == \"6\")|(Population_data[\"Syllables4\"] == \"5\")]\n",
    "            #print(selected_pops.index)\n",
    "###########            \n",
    "            Pop_data = pd.concat([Population_data,words_array2],axis=1)\n",
    "            # print()\n",
    "            # print(\"PARENTS\")\n",
    "            # display(Pop_data)\n",
    "\n",
    "            return Pop_data\n",
    "\n",
    "    def child_poems(self,words_array,cross):\n",
    "        child_population = []\n",
    "        syllables_count = []\n",
    "        grammer_error = []\n",
    "        words_array_child = []\n",
    "        coherence_arr = []\n",
    "        # print(\"words_array\",words_array)\n",
    "        # print(type(words_array))\n",
    "        if len(words_array)%2 == 2:\n",
    "            for i in range(0,len(words_array)-1):\n",
    "                child1,child2,child1_poem,child2_poem = self.crossover(words_array[i],words_array[i+1],cross)\n",
    "                child_population.append(child1_poem)\n",
    "                child_population.append(child2_poem)\n",
    "                sum_syllables1 = self.store_syllables(child1)\n",
    "                sum_syllables2 = self.store_syllables(child2)\n",
    "                poem_error1 = self.Grammar_checker(child1_poem)\n",
    "                poem_error2 = self.Grammar_checker(child2_poem)\n",
    "                coherence1 = self.coherence_checker(child1_poem)\n",
    "                coherence2 = self.coherence_checker(child2_poem)\n",
    "                syllables_count.append(sum_syllables1)\n",
    "                syllables_count.append(sum_syllables2)\n",
    "                grammer_error.append(poem_error1)\n",
    "                grammer_error.append(poem_error2)\n",
    "                coherence_arr.append(coherence1)\n",
    "                coherence_arr.append(coherence2)\n",
    "                words_array_child.append(child1)\n",
    "                words_array_child.append(child2)\n",
    "        else:\n",
    "            for i in range(0,len(words_array)-1,2):\n",
    "                # print(words_array[i],words_array[i+1])\n",
    "                child1,child2,child1_poem,child2_poem = self.crossover(words_array[i],words_array[i+1],cross)\n",
    "                child_population.append(child1_poem)\n",
    "                child_population.append(child2_poem)\n",
    "                sum_syllables1 = self.store_syllables(child1)\n",
    "                sum_syllables2 = self.store_syllables(child2)\n",
    "                poem_error1 = self.Grammar_checker(child1_poem)\n",
    "                poem_error2 = self.Grammar_checker(child2_poem)\n",
    "                coherence1 = self.coherence_checker(child1_poem)\n",
    "                coherence2 = self.coherence_checker(child2_poem)\n",
    "                syllables_count.append(sum_syllables1)\n",
    "                syllables_count.append(sum_syllables2)\n",
    "                grammer_error.append(poem_error1)\n",
    "                grammer_error.append(poem_error2)\n",
    "                coherence_arr.append(coherence1)\n",
    "                coherence_arr.append(coherence2)\n",
    "                words_array_child.append(child1)\n",
    "                words_array_child.append(child2)\n",
    "        words_array_child = pd.DataFrame(dict(zip([\"words_array\"],[words_array_child])))\n",
    "        Final1 = np.hstack((child_population,syllables_count,grammer_error,coherence_arr))\n",
    "        child_data = pd.DataFrame(Final1,columns=[\"Line1\",\"Line2\",\"Line3\",\"Line4\",\"Syllables1\",\"Syllables2\",\"Syllables3\",\"Syllables4\",\"Syll_Count\",\"GrammarError\",\"Coherence\"])\n",
    "        \n",
    "######### Calculate Fitness of Child Poems\n",
    "        max_rank = len(child_data[\"Line1\"])\n",
    "#         child_data[\"Syllables1\"] = child_data[\"Syllables1\"].apply(lambda x: [int(el) for el in x.strip(\"[]\").split(\",\")])\n",
    "#         child_data[\"Syllables2\"] = child_data[\"Syllables2\"].apply(lambda x: [int(el) for el in x.strip(\"[]\").split(\",\")])\n",
    "#         child_data[\"Syllables3\"] = child_data[\"Syllables3\"].apply(lambda x: [int(el) for el in x.strip(\"[]\").split(\",\")])\n",
    "#         child_data[\"Syllables4\"] = child_data[\"Syllables4\"].apply(lambda x: [int(el) for el in x.strip(\"[]\").split(\",\")])\n",
    "        child_data[\"GrammarError\"] = child_data[\"GrammarError\"].apply(lambda x: [int(el) for el in x.strip(\"[]\").split(\",\")])\n",
    "        child_data[\"Gram_Rank\"] = child_data[\"GrammarError\"].rank(method = 'min', ascending = False)\n",
    "        child_data[\"Coherence\"] = child_data[\"Coherence\"].apply(lambda x: [float(el) for el in x.strip(\"[]\").split(\",\")])\n",
    "        child_data[\"Coh_Rank\"] = child_data[\"Coherence\"].rank(method = 'min')\n",
    "        #print(child_data.loc[1].at[\"Syllables1\"])\n",
    "        #print(type(child_data.loc[1].at[\"Syllables1\"]))\n",
    "        #child_data[\"Syll_Bonus\"] = child_data.apply(lambda row: 1 if (int(row.at[\"Syllables1\"]) == 5) else 0)\n",
    "        #child_data[\"Syll_Bonus\"] = child_data.apply(lambda row: 1 if ((row.Syllables1[0] == 5)&(row.Syllables2[0] == 6)&(row.Syllables3[0] == 6)&(row.Syllables4[0] == 5)) else 0, axis = 1)\n",
    "\n",
    "        child_data[\"Fitness\"] = child_data.apply(lambda row: 0.4*row.Gram_Rank/max_rank + 0.4*row.Coh_Rank/max_rank + 0.2*[row.Syll_Count], axis = 1)\n",
    "        child_data = child_data.explode(\"Coherence\")\n",
    "        child_data = child_data.explode(\"GrammarError\")\n",
    "########        \n",
    "        child_data = pd.concat([child_data,words_array_child],axis=1)\n",
    "        child_data = child_data.reset_index(drop=True)\n",
    "        print()\n",
    "        # print(\"CHILD\")\n",
    "        # display(child_data)\n",
    "        return child_data\n",
    "\n",
    "    def selection_from_child_parents(self,x,child_data,selected_Pop_data):\n",
    "        reduce = (len(list(child_data.index)) + len(list(selected_Pop_data.index))) - x\n",
    "        n = int(reduce/3)\n",
    "        p = reduce - n\n",
    "        if reduce > len(list(child_data.index)) + len(list(selected_Pop_data.index)):\n",
    "            print(\"ERROR\")\n",
    "        else:\n",
    "            if n > len(child_data):\n",
    "                n = len(child_data)\n",
    "                p = reduce - p\n",
    "            if n == 0:\n",
    "                n = 1\n",
    "                p = 1\n",
    "        d = pd.concat([selected_Pop_data,child_data],axis=0)\n",
    "        d = d.reset_index(drop=True)\n",
    "        # print(\"p:\",p)\n",
    "        # print(\"n:\",n)\n",
    "        random_number1 = random.sample(list(selected_Pop_data.index),p)\n",
    "        random_number2 = random.sample(list(range(len(list(selected_Pop_data.index)),len(list(d.index)))),n)\n",
    "        random_number1.extend(random_number2)\n",
    "        # print(random_number1)\n",
    "        for i in random_number1:\n",
    "          d.drop(i,axis=0,inplace = True)\n",
    "        d = d.reset_index(drop=True)\n",
    "        # print(\"NEW POPULATION\")\n",
    "        # display(d)\n",
    "        r = d[[\"Line1\",\"Line2\",\"Line3\",\"Line4\"]]\n",
    "        return d,r\n",
    "    \n",
    "    def __init__(self,words_df,n,cross,epoch,z):\n",
    "        # Set up arrays for storing values over epochs\n",
    "        avg_gram = [0]*(epoch+1)\n",
    "        avg_coh = [0]*(epoch+1)\n",
    "        best_gram = [0]*(epoch+1)\n",
    "        best_coh = [0]*(epoch+1)\n",
    "        perc_right_syll = []\n",
    "        \n",
    "        Selection_number = int(round(0.85*n))\n",
    "        best = []\n",
    "        assert n>1 ,f\"Population {n} cannot be less than or equal to 1\"\n",
    "        # assert n>=Selection_number, f\"Cannot select {Selection_number} poems from total population of {n}\"\n",
    "        assert cross>1, f\"Minimum number for crossover should be 1\"\n",
    "        # assert 2*n >= Selection_number ,f\"Cannot make initial population of {n} if only {Selection_number} parents are selected\"\n",
    "\n",
    "        Population_data = self.create_set_of_poems(n)\n",
    "        avg_gram[0] = Population_data[\"GrammarError\"].mean()\n",
    "        avg_coh[0] = Population_data[\"Coherence\"].mean()\n",
    "        best_gram[0] = Population_data[\"GrammarError\"].min()\n",
    "        best_coh[0] = Population_data[\"Coherence\"].max()\n",
    "\n",
    "        for i in range(epoch):\n",
    "            Selected_Population_data,selected_polulation_word_array = self.tournament(Population_data,Selection_number,best)\n",
    "            child_population = self.child_poems(selected_polulation_word_array,cross)\n",
    "            final_population,final_poems = self.selection_from_child_parents(n,child_population,Population_data)\n",
    "            avg_gram[i+1] = final_population[\"GrammarError\"].mean()\n",
    "            avg_coh[i+1] = final_population[\"Coherence\"].mean()\n",
    "            best_gram[i+1] = Population_data[\"GrammarError\"].min()\n",
    "            best_coh[i+1] = Population_data[\"Coherence\"].max()\n",
    "            if i != epoch-1:\n",
    "                if (i+1)%10 == 0: # User feedback every 10 epochs\n",
    "                    print(\"Select best poems\")\n",
    "                    display(final_poems)\n",
    "                    best = (list(map(int,input(\"\\n Enter best poems numbers: \").strip().split())))[:z]\n",
    "                    #print(best)\n",
    "                Population_data = final_population\n",
    "                print(\"Final poems\")\n",
    "                display(final_population)\n",
    "\n",
    "            else:\n",
    "                print(\"###################################################\")\n",
    "                print(\"Last Poems Standing\")\n",
    "                print(\"###################################################\")\n",
    "                display(final_population)\n",
    "                \n",
    "        print(\"Average Grammar\")\n",
    "        print(avg_gram)\n",
    "        print(\"Best Grammar\")\n",
    "        print(best_gram)\n",
    "        print(\"Average Coherence\")\n",
    "        print(avg_coh)\n",
    "        print(\"Best Coherence\")\n",
    "        print(best_coh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                   Line1                         Line2  \\\n",
      "0                      happy songs of is                  just to of i   \n",
      "1             mire remembering your your     pleasure popularity we of   \n",
      "2                    that walk know they          lifting in fell case   \n",
      "3                voice warplanes my what         before us past enough   \n",
      "4                    ocean is is dialled      speaks natures only your   \n",
      "5                      field of spent in       we bridges eyes blowins   \n",
      "6             tear delusions him nothing       home gracing with child   \n",
      "7                     not human like the            we gray light dont   \n",
      "8                        the up chime in            in know spaces all   \n",
      "9                    solved think has it        by oneway stray belong   \n",
      "10                     sneeze show of to      pursued in bladeend life   \n",
      "11                  crass letting and we         revealing eggs are or   \n",
      "12                     and i tell chores               i with into out   \n",
      "13                          and i so the      and loneliness arena the   \n",
      "14               our day sanctify almost              for im to bottom   \n",
      "15             men first together gather         the after words round   \n",
      "16                doubt squeezed we your             our and skirt was   \n",
      "17                  garden reason to the             heard too they to   \n",
      "18                   spells zoom love he          the called we perish   \n",
      "19                   here kill man apace        behaviour where day is   \n",
      "20                    and all beasts and         that the discloses we   \n",
      "21                     ever stare and of           winter how here far   \n",
      "22       humankind faster ambulances and          nothing nature a the   \n",
      "23            hearts alive morning fruit  intensity hope you something   \n",
      "24                    the no smother the          we who battle sparks   \n",
      "25                was divorce when sings                  it the i all   \n",
      "26                     tell joy it every            race gods cake but   \n",
      "27                          is know is a        from their air matters   \n",
      "28                 definte life face the                to fruit be of   \n",
      "29                   and now rainbow now              is sea except to   \n",
      "30                   i out real everyone              life door i will   \n",
      "31                  the time say swallow                  the i too us   \n",
      "32                  the follows said the       dream nottoo change and   \n",
      "33              else sequin think circle           stoke to farm other   \n",
      "34             saw hope heavenly couldnt            we in have believe   \n",
      "35                   from reeled as will          result united in one   \n",
      "36                      is from much out               the of to roars   \n",
      "37  escaped expressing breaching holding        to flurry believe fast   \n",
      "38                    glow for be reeled               we fears one of   \n",
      "39                  last roof meant they  babble eventually nettle but   \n",
      "40     swallowed winter travelling gives     is those resistance panes   \n",
      "41                     life nor hour hoe              me over wear the   \n",
      "42                     going not in look         mankind breathe we is   \n",
      "43                  as nothing not today            far off of essence   \n",
      "44                    our been high have        maybe strike from tree   \n",
      "45                      to of stands not                the with is of   \n",
      "46                    of two happy patch            not in where times   \n",
      "47           bare all standstill refined            have told black in   \n",
      "48                    such in light soul    arms night but passionless   \n",
      "49               it that belongs finally            from of music were   \n",
      "\n",
      "                              Line3                                 Line4  \\\n",
      "0            this lifting the there                  cant road losers you   \n",
      "1          breast part offered ants         insights are hope environment   \n",
      "2            us everything a crying                  gaily secret on less   \n",
      "3           thoughts he hudsons now                     trees log put and   \n",
      "4        primordial in and slippery                   come life there the   \n",
      "5                   at or stoke had                    all melt night all   \n",
      "6                 human now soon we                 answers i winters the   \n",
      "7                out long souls out                  they well appear are   \n",
      "8            of bees positivity and                  experience on my the   \n",
      "9          air has black everything          his no murmuring destination   \n",
      "10  heels impoverished unable tight                   neck of clapped the   \n",
      "11              the last misses the                   a kindred it chance   \n",
      "12          antennae me future dust                 of create state drown   \n",
      "13                 for was dust the                    maid it time given   \n",
      "14             thought not sing the                    double to who seas   \n",
      "15                it saw clothes to              shadows there living day   \n",
      "16                a how the without                       head was the my   \n",
      "17         we up they selfappointed                   i with wake sitting   \n",
      "18                    my of flow of                  to with spider heard   \n",
      "19                 the words full a              travellers must but tear   \n",
      "20       destination little in what         undermining facebook wild the   \n",
      "21                  be but come two                         will a had to   \n",
      "22             seemed pang with the                  around mist frost we   \n",
      "23          matrix sandy gives have                and winter world lotsa   \n",
      "24      ice choristers neck winners                     no depends we our   \n",
      "25                 they of have the                   we the should those   \n",
      "26                   to the try and                         and it way or   \n",
      "27       inclination is juggling on                          the to in it   \n",
      "28      and prison shades romancing           descent mine scarlet thrown   \n",
      "29                       of of by i                   day gloom human are   \n",
      "30                 im the say heard                 feelings of come soul   \n",
      "31                    if love or of  songs mind differently budinspecting   \n",
      "32               destiny two time a            tender beer winter unknown   \n",
      "33          equation heavy im wings            combining in falling night   \n",
      "34               reports than to we                  aware one flurry are   \n",
      "35          next and squashed spell                    shuts me white all   \n",
      "36                   as tis i arise                    to their have hope   \n",
      "37         delightful a window this               fell forever star ghost   \n",
      "38            and rooms stays frost              of does warmongers quiet   \n",
      "39            dreams the tree those                        lack way it to   \n",
      "40              out are deaths hope         fill tomorrow yankees hemlock   \n",
      "41          happened just though us            by everything its thinking   \n",
      "42                  what over at in           pink looking are everything   \n",
      "43                  here we on come               not holding and succumb   \n",
      "44        embalm follows that climb                      we under less in   \n",
      "45             pink inside this are                    stay time life has   \n",
      "46                see like owe room         misery gone kite departmental   \n",
      "47               grief the for eyes                   for speak the round   \n",
      "48           when both the thoughts                  across that which to   \n",
      "49               are felt desert of                       where over a in   \n",
      "\n",
      "   Syllables1 Syllables2 Syllables3 Syllables4 Syll_Count GrammarError  \\\n",
      "0           5          4          6          5      [2.0]         [17]   \n",
      "1           7         10          6          9      [0.0]         [26]   \n",
      "2           4          5          7          6      [0.0]         [18]   \n",
      "3           7          7          5          4      [0.0]         [24]   \n",
      "4           6          7          8          5      [1.0]         [25]   \n",
      "5           4          6          5          4      [2.0]         [12]   \n",
      "6           7          5          5          6      [0.0]         [20]   \n",
      "7           5          4          4          6      [1.0]         [12]   \n",
      "8           5          5          8          7      [1.0]         [17]   \n",
      "9           5          7          7          9      [1.0]         [14]   \n",
      "10          5          6         10          5      [4.0]         [22]   \n",
      "11          5          7          5          6      [1.0]         [18]   \n",
      "12          5          5          8          6      [1.0]         [14]   \n",
      "13          4          9          4          5      [1.0]         [13]   \n",
      "14          7          5          4          5      [1.0]         [19]   \n",
      "15          7          5          5          7      [0.0]         [15]   \n",
      "16          5          4          5          4      [1.0]         [13]   \n",
      "17          6          4          7          5      [1.0]         [14]   \n",
      "18          4          6          4          5      [3.0]         [13]   \n",
      "19          7          7          4          6      [0.0]         [26]   \n",
      "20          4          6          8          9      [2.0]         [29]   \n",
      "21          6          6          4          4      [2.0]         [16]   \n",
      "22         10          7          5          5      [1.0]         [28]   \n",
      "23          7          9          7          6      [0.0]         [24]   \n",
      "24          5          5          8          5      [2.0]         [10]   \n",
      "25          6          4          4          5      [1.0]         [11]   \n",
      "26          6          4          4          4      [0.0]         [13]   \n",
      "27          4          5          8          4      [0.0]         [20]   \n",
      "28          6          4          8          6      [0.0]         [19]   \n",
      "29          5          5          4          6      [1.0]         [10]   \n",
      "30          7          4          4          5      [1.0]         [15]   \n",
      "31          5          4          4         10      [1.0]         [13]   \n",
      "32          5          6          6          7      [3.0]         [21]   \n",
      "33          7          6          7          7      [2.0]         [26]   \n",
      "34          6          6          5          9      [2.0]         [17]   \n",
      "35          5          8          5          5      [2.0]         [19]   \n",
      "36          4          4          6          4      [0.0]          [9]   \n",
      "37         10          7          7          6      [0.0]         [23]   \n",
      "38          5          5          4          6      [1.0]         [11]   \n",
      "39          4          9          5          4      [0.0]         [20]   \n",
      "40         10          9          5          8      [0.0]         [18]   \n",
      "41          4          5          6          8      [0.0]         [16]   \n",
      "42          4          6          5          9      [2.0]         [20]   \n",
      "43          6          6          5          6      [2.0]         [12]   \n",
      "44          4          6          6          5      [3.0]         [16]   \n",
      "45          4          4          7          4      [0.0]         [13]   \n",
      "46          5          6          4          9      [3.0]         [14]   \n",
      "47          7          4          4          4      [0.0]         [22]   \n",
      "48          4          6          4          5      [3.0]         [18]   \n",
      "49          7          6          6          6      [2.0]         [14]   \n",
      "\n",
      "                   Coherence  Gram_Rank  Coh_Rank  \n",
      "0                      [0.0]       24.0       1.0  \n",
      "1                      [0.0]        3.0       1.0  \n",
      "2   [1.0179847363368634e-07]       20.0      20.0  \n",
      "3   [2.4884071332678885e-07]        7.0      22.0  \n",
      "4    [2.199574162442151e-06]        6.0      37.0  \n",
      "5    [2.199574162442151e-06]       43.0      37.0  \n",
      "6    [6.221017833169721e-07]       13.0      29.0  \n",
      "7   [4.6657633748772904e-06]       43.0      44.0  \n",
      "8                      [0.0]       24.0       1.0  \n",
      "9                      [0.0]       32.0       1.0  \n",
      "10                     [0.0]       10.0       1.0  \n",
      "11                     [0.0]       20.0       1.0  \n",
      "12                     [0.0]       32.0       1.0  \n",
      "13  [1.1197832099705497e-05]       37.0      48.0  \n",
      "14   [6.221017833169721e-07]       17.0      29.0  \n",
      "15                     [0.0]       30.0       1.0  \n",
      "16                     [0.0]       37.0       1.0  \n",
      "17                     [0.0]       32.0       1.0  \n",
      "18                     [0.0]       37.0       1.0  \n",
      "19  [1.4930442799607329e-06]        3.0      36.0  \n",
      "20                     [0.0]        1.0       1.0  \n",
      "21   [1.045130995972513e-05]       27.0      47.0  \n",
      "22   [7.465221399803664e-07]        2.0      31.0  \n",
      "23   [5.024668249867851e-07]        7.0      27.0  \n",
      "24                     [0.0]       48.0       1.0  \n",
      "25                     [0.0]       46.0       1.0  \n",
      "26  [3.9992257498948204e-07]       37.0      25.0  \n",
      "27  [2.2395664199410993e-06]       13.0      39.0  \n",
      "28   [3.664745050812708e-06]       17.0      42.0  \n",
      "29  [2.6128274899312825e-06]       48.0      41.0  \n",
      "30   [9.704787819744763e-06]       30.0      46.0  \n",
      "31  [1.5552544582924302e-07]       37.0      21.0  \n",
      "32  [1.4421450431438899e-06]       12.0      35.0  \n",
      "33   [2.368772174937701e-06]        3.0      40.0  \n",
      "34    [4.66576337487729e-07]       24.0      26.0  \n",
      "35                     [0.0]       17.0       1.0  \n",
      "36                     [0.0]       50.0       1.0  \n",
      "37                     [0.0]        9.0       1.0  \n",
      "38                     [0.0]       46.0       1.0  \n",
      "39   [3.732610699901832e-07]       13.0      24.0  \n",
      "40   [5.089923681684316e-07]       20.0      28.0  \n",
      "41  [1.3219662895485657e-05]       27.0      49.0  \n",
      "42  [4.2658407998878084e-06]       13.0      43.0  \n",
      "43   [7.998451499789641e-07]       43.0      32.0  \n",
      "44  [1.1997677249684462e-06]       27.0      33.0  \n",
      "45  [4.6657633748772906e-05]       37.0      50.0  \n",
      "46   [2.544961840842158e-07]       32.0      23.0  \n",
      "47  [1.2442035666339441e-06]       10.0      34.0  \n",
      "48    [5.33230099985976e-06]       20.0      45.0  \n",
      "49                     [0.0]       32.0       1.0  \n",
      "<class 'list'>\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can't multiply sequence by non-int of type 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/15/pg1xsxrd4xz368rbkkk77qv80000gn/T/ipykernel_57169/2826632313.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#4 (words_df,intial_population,crossover,epochs,user_selection_poems_number):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpoem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpoem_generation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/15/pg1xsxrd4xz368rbkkk77qv80000gn/T/ipykernel_57169/2528492045.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, words_df, n, cross, epoch, z)\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;31m# assert 2*n >= Selection_number ,f\"Cannot make initial population of {n} if only {Selection_number} parents are selected\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 358\u001b[0;31m         \u001b[0mPopulation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_set_of_poems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m         \u001b[0mavg_gram\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPopulation_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"GrammarError\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m         \u001b[0mavg_coh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPopulation_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Coherence\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/15/pg1xsxrd4xz368rbkkk77qv80000gn/T/ipykernel_57169/2528492045.py\u001b[0m in \u001b[0;36mcreate_set_of_poems\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    221\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPopulation_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPopulation_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Syll_Count\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m             \u001b[0mPopulation_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Fitness\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPopulation_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0.4\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGram_Rank\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mmax_rank\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0.4\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCoh_Rank\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mmax_rank\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSyll_Count\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m             \u001b[0mPopulation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPopulation_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Coherence\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m             \u001b[0mPopulation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPopulation_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"GrammarError\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, axis, raw, result_type, args, **kwargs)\u001b[0m\n\u001b[1;32m   8738\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8739\u001b[0m         )\n\u001b[0;32m-> 8740\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   8741\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8742\u001b[0m     def applymap(\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    686\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_raw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    811\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 812\u001b[0;31m         \u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_series_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    813\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    814\u001b[0m         \u001b[0;31m# wrap results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_series_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    826\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseries_gen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m                 \u001b[0;31m# ignore SettingWithCopy here in case the user mutates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m                 \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mABCSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m                     \u001b[0;31m# If we have a view on v, we need to make a copy because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/15/pg1xsxrd4xz368rbkkk77qv80000gn/T/ipykernel_57169/2528492045.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(row)\u001b[0m\n\u001b[1;32m    221\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPopulation_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPopulation_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Syll_Count\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m             \u001b[0mPopulation_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Fitness\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPopulation_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0.4\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGram_Rank\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mmax_rank\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0.4\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCoh_Rank\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mmax_rank\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSyll_Count\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m             \u001b[0mPopulation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPopulation_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Coherence\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m             \u001b[0mPopulation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPopulation_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"GrammarError\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: can't multiply sequence by non-int of type 'float'"
     ]
    }
   ],
   "source": [
    "#4 (words_df,intial_population,crossover,epochs,user_selection_poems_number):\n",
    "poem = poem_generation(words_df,50,2,10,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(range(5,8)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [2,3,4,5]\n",
    "\n",
    "for i in a:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cc5f70855ac006f3de45a3cc3b9e7d8d53845e50458809cb162b0174266dec97"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
